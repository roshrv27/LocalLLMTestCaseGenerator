# ğŸš€ Proj1_LocalLLMTestCaseGenerator

A local AI-powered test case generator that uses **Ollama** and **Llama 3.2** to automatically create comprehensive test cases from natural language requirements.

![Python](https://img.shields.io/badge/Python-3.9+-blue?logo=python)
![Flask](https://img.shields.io/badge/Flask-3.x-green?logo=flask)
![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-orange)
![License](https://img.shields.io/badge/License-MIT-yellow)

---

## âœ¨ Features

- ğŸ¤– **AI-Powered**: Uses Llama 3.2 (3B) running locally via Ollama
- ğŸ”’ **Privacy-First**: All processing happens on your machine - no data leaves your computer
- ğŸ’¬ **Chat Interface**: Modern, dark-mode UI for intuitive interaction
- ğŸ“ **Markdown Output**: Test cases rendered in clean, formatted Markdown
- âš¡ **Real-time**: Instant test case generation with typing indicators

---

## ğŸ—ï¸ Architecture

```mermaid
graph TD
    classDef browser fill:#f9f9f9,stroke:#333,stroke-width:2px;
    classDef server fill:#e1f5fe,stroke:#0277bd,stroke-width:2px;
    classDef ai fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px;

    subgraph UserSpace ["ğŸŒ User Space"]
        Browser["ğŸ’» Browser (React UI)"]:::browser
    end

    subgraph Backend ["âš™ï¸ Backend Server"]
        Flask["ğŸ”¥ Flask API\n(Port 3000)"]:::server
        PromptEng["ğŸ“ Prompt Engine"]:::server
    end

    subgraph LocalAI ["ğŸ¦™ Local AI Cluster"]
        Ollama["ğŸ§  Ollama Runtime\n(Port 11434)"]:::ai
        Llama["ğŸ’¿ Llama 3.2 Model"]:::ai
    end

    Browser <-->|"HTTP POST /generate"| Flask
    Flask -->|Build Prompt| PromptEng
    PromptEng -->|Structured Context| Flask
    Flask <-->|Generate Request| Ollama
    Ollama <-->|Inference| Llama
```

### Flow Diagram

```mermaid
sequenceDiagram
    participant User
    participant Browser
    participant Flask
    participant PromptEngine
    participant OllamaClient
    participant Llama3.2

    User->>Browser: Enter requirements
    Browser->>Flask: POST /generate {user_input}
    Flask->>PromptEngine: build_prompt(user_input)
    PromptEngine-->>Flask: Structured prompt
    Flask->>OllamaClient: generate_response(prompt)
    OllamaClient->>Llama3.2: ollama.chat()
    Llama3.2-->>OllamaClient: Test cases (Markdown)
    OllamaClient-->>Flask: Response text
    Flask-->>Browser: JSON {test_cases_markdown}
    Browser->>Browser: marked.parse() â†’ HTML
    Browser-->>User: Display formatted test cases
```

---

## ğŸ“ Project Structure

```
Proj1_LocalLLMTestCaseGenerator/
â”œâ”€â”€ app.py                    # Flask backend server
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ ollama_client.py      # Ollama API wrapper
â”‚   â””â”€â”€ prompt_engine.py      # Prompt template builder
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html            # Chat UI template
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ styles.css        # Dark mode styling
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ script.js         # Frontend logic
â”œâ”€â”€ architecture/
â”‚   â””â”€â”€ sops.md               # System SOPs
â”œâ”€â”€ task_plan.md              # Project task tracker
â”œâ”€â”€ progress.md               # Development log
â”œâ”€â”€ findings.md               # Research & discoveries
â”œâ”€â”€ gemini.md                 # Data schemas & rules
â”œâ”€â”€ B.L.A.S.T.md              # Development protocol
â””â”€â”€ .gitignore
```

---

## ğŸ› ï¸ Prerequisites

1. **Python 3.9+**
2. **Ollama** installed and running
   ```bash
   # Install Ollama (macOS)
   brew install ollama
   
   # Or download from https://ollama.ai
   ```

3. **Llama 3.2 model**
   ```bash
   ollama pull llama3.2
   ```

---

## ğŸš€ Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/roshrv27/Proj1_LocalLLMTestCaseGenerator.git
   cd Proj1_LocalLLMTestCaseGenerator
   ```

2. **Install dependencies**
   ```bash
   pip install flask ollama
   ```

3. **Start Ollama** (in a separate terminal)
   ```bash
   ollama serve
   ```

4. **Run the application**
   ```bash
   python3 app.py
   ```

5. **Open your browser**
   ```
   http://localhost:8080
   ```

---

## ğŸ’¡ Usage

1. Open the web interface at `http://localhost:8080`
2. Type your feature requirements in natural language:
   ```
   Login page with email and password fields, 
   forgot password link, and remember me checkbox
   ```
3. Press Enter or click Send
4. Receive structured test cases including:
   - Test Case ID
   - Description
   - Pre-conditions
   - Test Steps
   - Expected Results
   - Positive & Negative scenarios

---

## ğŸ¨ Screenshots

### Chat Interface
The modern dark-mode interface provides a clean, distraction-free environment for generating test cases.

### Sample Output
```markdown
## TC_001: Verify successful login with valid credentials
**Pre-conditions:** User account exists
**Steps:**
1. Navigate to login page
2. Enter valid email
3. Enter valid password
4. Click Login button
**Expected Result:** User is redirected to dashboard
```

---

## ğŸ”§ Configuration

| Setting | Location | Default |
|---------|----------|---------|
| Port | `app.py` | 8080 |
| Model | `tools/ollama_client.py` | llama3.2 |
| Host | `app.py` | 0.0.0.0 |

---

## ğŸ“„ License

MIT License - feel free to use this project for personal or commercial purposes.

---

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai) - Local LLM runtime
- [Meta Llama](https://llama.meta.com) - Llama 3.2 model
- [Flask](https://flask.palletsprojects.com) - Python web framework
- [Marked.js](https://marked.js.org) - Markdown parser

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

**Built with â¤ï¸ using the B.L.A.S.T. Protocol**
