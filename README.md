# ğŸš€ Proj1_LocalLLMTestCaseGenerator

A local AI-powered test case generator that uses **Ollama** and **Llama 3.2** to automatically create comprehensive test cases from natural language requirements.

![Python](https://img.shields.io/badge/Python-3.9+-blue?logo=python)
![Flask](https://img.shields.io/badge/Flask-3.x-green?logo=flask)
![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-orange)
![License](https://img.shields.io/badge/License-MIT-yellow)

---

## âœ¨ Features

- ğŸ¤– **AI-Powered**: Uses Llama 3.2 (3B) running locally via Ollama
- ğŸ”’ **Privacy-First**: All processing happens on your machine - no data leaves your computer
- ğŸ’¬ **Chat Interface**: Modern, dark-mode UI for intuitive interaction
- ğŸ“ **Markdown Output**: Test cases rendered in clean, formatted Markdown
- âš¡ **Real-time**: Instant test case generation with typing indicators

---

## ğŸ—ï¸ Architecture

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                              ğŸŒ USER BROWSER                                  â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚                         Chat Interface (UI)                             â”‚  â•‘
â•‘  â”‚                                                                         â”‚  â•‘
â•‘  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  â•‘
â•‘  â”‚   â”‚    Input     â”‚ â”€â”€â”€â–¶ â”‚    Send      â”‚ â”€â”€â”€â–¶ â”‚  Display Test    â”‚     â”‚  â•‘
â•‘  â”‚   â”‚    Box       â”‚      â”‚    Button    â”‚      â”‚  Cases (MD)      â”‚     â”‚  â•‘
â•‘  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â•‘
â•‘  â”‚                                                                         â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                       â”‚
                                       â”‚  HTTP POST /generate
                                       â”‚  { "user_input": "..." }
                                       â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                              âš™ï¸  FLASK BACKEND                                â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚  app.py                                                                 â”‚  â•‘
â•‘  â”‚                                                                         â”‚  â•‘
â•‘  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â•‘
â•‘  â”‚   â”‚   /generate        â”‚  â”€â”€â”€â”€â–¶  â”‚   prompt_engine.py             â”‚    â”‚  â•‘
â•‘  â”‚   â”‚   endpoint         â”‚         â”‚   (Build structured prompt)    â”‚    â”‚  â•‘
â•‘  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â•‘
â•‘  â”‚                                                                         â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                       â”‚
                                       â”‚  ollama.chat(model, messages)
                                       â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                              ğŸ¦™ OLLAMA (Local)                                â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚  ollama_client.py  â”€â”€â”€â”€â”€â”€â”€â”€â–¶  http://127.0.0.1:11434                    â”‚  â•‘
â•‘  â”‚                                                                         â”‚  â•‘
â•‘  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â•‘
â•‘  â”‚   â”‚                                                               â”‚    â”‚  â•‘
â•‘  â”‚   â”‚   ğŸ¦™  Llama 3.2                                               â”‚    â”‚  â•‘
â•‘  â”‚   â”‚   â”œâ”€â”€ Parameters: 3.2B                                        â”‚    â”‚  â•‘
â•‘  â”‚   â”‚   â”œâ”€â”€ Quantization: Q4_K_M                                    â”‚    â”‚  â•‘
â•‘  â”‚   â”‚   â””â”€â”€ Size: ~2GB                                              â”‚    â”‚  â•‘
â•‘  â”‚   â”‚                                                               â”‚    â”‚  â•‘
â•‘  â”‚   â”‚   ğŸ“¥ Input:  Structured prompt with user requirements         â”‚    â”‚  â•‘
â•‘  â”‚   â”‚   ğŸ“¤ Output: Markdown-formatted test cases                    â”‚    â”‚  â•‘
â•‘  â”‚   â”‚                                                               â”‚    â”‚  â•‘
â•‘  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â•‘
â•‘  â”‚                                                                         â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Flow Diagram

```mermaid
sequenceDiagram
    participant User
    participant Browser
    participant Flask
    participant PromptEngine
    participant OllamaClient
    participant Llama3.2

    User->>Browser: Enter requirements
    Browser->>Flask: POST /generate {user_input}
    Flask->>PromptEngine: build_prompt(user_input)
    PromptEngine-->>Flask: Structured prompt
    Flask->>OllamaClient: generate_response(prompt)
    OllamaClient->>Llama3.2: ollama.chat()
    Llama3.2-->>OllamaClient: Test cases (Markdown)
    OllamaClient-->>Flask: Response text
    Flask-->>Browser: JSON {test_cases_markdown}
    Browser->>Browser: marked.parse() â†’ HTML
    Browser-->>User: Display formatted test cases
```

---

## ğŸ“ Project Structure

```
Proj1_LocalLLMTestCaseGenerator/
â”œâ”€â”€ app.py                    # Flask backend server
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ ollama_client.py      # Ollama API wrapper
â”‚   â””â”€â”€ prompt_engine.py      # Prompt template builder
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html            # Chat UI template
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ styles.css        # Dark mode styling
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ script.js         # Frontend logic
â”œâ”€â”€ architecture/
â”‚   â””â”€â”€ sops.md               # System SOPs
â”œâ”€â”€ task_plan.md              # Project task tracker
â”œâ”€â”€ progress.md               # Development log
â”œâ”€â”€ findings.md               # Research & discoveries
â”œâ”€â”€ gemini.md                 # Data schemas & rules
â”œâ”€â”€ B.L.A.S.T.md              # Development protocol
â””â”€â”€ .gitignore
```

---

## ğŸ› ï¸ Prerequisites

1. **Python 3.9+**
2. **Ollama** installed and running
   ```bash
   # Install Ollama (macOS)
   brew install ollama
   
   # Or download from https://ollama.ai
   ```

3. **Llama 3.2 model**
   ```bash
   ollama pull llama3.2
   ```

---

## ğŸš€ Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/roshrv27/Proj1_LocalLLMTestCaseGenerator.git
   cd Proj1_LocalLLMTestCaseGenerator
   ```

2. **Install dependencies**
   ```bash
   pip install flask ollama
   ```

3. **Start Ollama** (in a separate terminal)
   ```bash
   ollama serve
   ```

4. **Run the application**
   ```bash
   python3 app.py
   ```

5. **Open your browser**
   ```
   http://localhost:8080
   ```

---

## ğŸ’¡ Usage

1. Open the web interface at `http://localhost:8080`
2. Type your feature requirements in natural language:
   ```
   Login page with email and password fields, 
   forgot password link, and remember me checkbox
   ```
3. Press Enter or click Send
4. Receive structured test cases including:
   - Test Case ID
   - Description
   - Pre-conditions
   - Test Steps
   - Expected Results
   - Positive & Negative scenarios

---

## ğŸ¨ Screenshots

### Chat Interface
The modern dark-mode interface provides a clean, distraction-free environment for generating test cases.

### Sample Output
```markdown
## TC_001: Verify successful login with valid credentials
**Pre-conditions:** User account exists
**Steps:**
1. Navigate to login page
2. Enter valid email
3. Enter valid password
4. Click Login button
**Expected Result:** User is redirected to dashboard
```

---

## ğŸ”§ Configuration

| Setting | Location | Default |
|---------|----------|---------|
| Port | `app.py` | 8080 |
| Model | `tools/ollama_client.py` | llama3.2 |
| Host | `app.py` | 0.0.0.0 |

---

## ğŸ“„ License

MIT License - feel free to use this project for personal or commercial purposes.

---

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai) - Local LLM runtime
- [Meta Llama](https://llama.meta.com) - Llama 3.2 model
- [Flask](https://flask.palletsprojects.com) - Python web framework
- [Marked.js](https://marked.js.org) - Markdown parser

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

**Built with â¤ï¸ using the B.L.A.S.T. Protocol**
